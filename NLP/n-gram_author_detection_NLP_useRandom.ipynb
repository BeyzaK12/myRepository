{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beyza_Kurt_NLP_final_part1_CENG3526.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEfgPOu5grRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io, os\n",
        "import re as re\n",
        "import zipfile as zipfile\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from operator import itemgetter\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import scipy.spatial.distance\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYaKz15qZonk",
        "colab_type": "code",
        "outputId": "794652dc-552b-4ab6-f06a-d37202ad39b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN8iG5JYgzdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textsDocNumbers = {}       # [ doc: text, ... ]\n",
        "authorsTextNumbers = {}         # to split train and test data\n",
        "\n",
        "mytextzip = ''\n",
        "path = '/content/drive/My Drive/30Columnists.zip'\n",
        "\n",
        "with zipfile.ZipFile(path) as z:\n",
        "    for zipinfo in z.infolist():\n",
        "        if zipinfo.filename.endswith('.txt') and re.search('raw_texts', zipinfo.filename):\n",
        "            with z.open(zipinfo) as f:\n",
        "                textfile = io.TextIOWrapper(f, encoding='cp1254', newline='')\n",
        "                for line in textfile:\n",
        "                    if len(line.strip()): mytextzip += line.strip()\n",
        "\n",
        "                filename = str(zipinfo.filename)\n",
        "                filename = filename.split(\"/\")\n",
        "                filename = filename[-1][:-4]\n",
        "\n",
        "                mytextzip = mytextzip.lower()\n",
        "                \n",
        "                textsDocNumbers[filename] = mytextzip\n",
        "\n",
        "                author, doc = filename.split(\".\")\n",
        "                if author not in authorsTextNumbers:\n",
        "                    authorsTextNumbers[author] = 1\n",
        "                else:\n",
        "                    authorsTextNumbers[author] += 1\n",
        "\n",
        "                mytextzip = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phldmUrZoho7",
        "colab_type": "code",
        "outputId": "846a0477-bf61-4e70-dd5c-42f3162ca9c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "for doc, text in textsDocNumbers.items():\n",
        "    print(\"key: \" + doc + \"\\nvalue: \" + text)\n",
        "    break"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "key: 1.1\n",
            "value: there can be no doubt that edinburgh is a huge tourist attraction both within the united kingdom and internationally. there is so much to see in scotland, but its capital is arguably its biggest gateway if not its biggest draw.capitals are not always that lucky. i have never met anyone who has, by choice, travelled to australia to visit canberra  or to peru to see lima.although it can sometimes be difficult on a driech march morning when the wind is howling up your trouser legs and the drizzle is misting up your spectacles, edinburgh is an often idyllic, romantic and fun-packed destination.i remember being amazed a couple of decades ago when some english friends of mine told me they were coming to edinburgh on their honeymoon  i had never appreciated my home as a place for newlyweds to choose for their first married tryst. now edinburgh is in the multi-million pound premier league for british honeymoons  and why not?tourism has become a hugely important sector of the edinburgh economy  thanks not just to the international festival of the arts and its associated fringe, but also the all-year-round entertainment that it offers from theatres, sporting events, galleries, museums and the associated consumption of fine food and alcohol. add to this cocktail  which we have to admit many other cities also have  is the wonder of edinburgh's built environment, the richness of its historic tapestry and beauty of its natural hills, lochs and parks. such a combination is what puts edinburgh second only to london as the uk's biggest draw.it also has leith  which with three michelin-starred restaurants has more exceptional eateries than glasgow, manchester and liverpool put together.in such a context it is therefore vital that how edinburgh is marketed is carefully thought through and ensures that it can pull as many visitors  even if it is on their way to another destination. up until now the city's approach has been to pitch in with the general marketing of scotland as a destination so that visitors at the very least pass through  and hopefully stay.just as scotland has relied on british tourism marketing for the reason that the vast majority of scotland's foreign visitors arrive in london before coming north, so edinburgh relied on using the scottish tourist board  now visitscotland.the opening up of the skies to greater low cost competitive travel has eaten away at this strategy within the short-haul routes, but for transatlantic and other long-haul flights, the principal remains sound.the problem for edinburgh is that the rest of scotland takes the same approach  with all the other councils such as glasgow and aberdeen pooling their resources too. the difficulty is that visitscotland is pulled all ways and some of the smaller tourist destinations get very upset if they don't get any support. the result? edinburgh council has decided to pull out some half a million pounds of funding from visitscotland and glasgow and stornoway have done likewise.i can't help but think this is a mistake. i managed the marketing of one of edinburgh's premier attractions for three years, drawing up the budgets and spending the pennies from bus adverts to world-wide listings and i can tell you half a million will get us diddley squat.it still has to be remembered that visitscotland is funded by the scottish government to the tune of a whopping £42 million  that's a lot of money that we now have less influence over.i don't know what the council really thinks it will do instead but i have every reason to believe that this decision  whilst on the face of it looking attractive  is going to prove very costly in the end. the people that will end up paying for it will be the overburdened council taxpayer  who i expect to be asked to cough up more  and the hoteliers, visitor attractions and other businesses that could see edinburgh's marketing be under–resourced just when a big push is needed most  during a recession.a bigger debate is needed about this policy change with a fuller explanation of why it is good for the city and what, if it doesn't work, the councillors that have taken this decision will do instead. do they have a plan b?at a time when edinburgh's financial services industry is in serious trouble and losing jobs, the last thing the city needs is for its own councillors to drop the ball and cause more unemployment for those that live of the munificence of tourism.a bunch of walliesat a time when scots are sucking their gums wondering what next financial threat is about to befall them, the decision of the parliament to spend money on inscribing more verse on the a 'canongate wall' to commemorate the first ten years of devolution is an insult to our intelligence. a better idea would be to brick up the entrances and exits, preferably with most of the msps still inside.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lvvF7K8UeDb",
        "colab_type": "code",
        "outputId": "aec0cbcb-1f82-4817-8fdd-d4ce7511a6b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(authorsTextNumbers)"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'1': 50, '10': 50, '11': 50, '12': 50, '13': 50, '14': 50, '15': 50, '16': 50, '17': 50, '18': 50, '19': 50, '2': 50, '20': 50, '21': 50, '22': 50, '23': 50, '24': 50, '25': 50, '26': 50, '27': 50, '28': 50, '29': 50, '3': 50, '30': 50, '4': 50, '5': 50, '6': 50, '7': 50, '8': 50, '9': 50}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jglEIQj73DN_",
        "colab": {}
      },
      "source": [
        "def splitTrainTest(dict, n):\n",
        "    train = {}      # { author: [text1's doc number, text2's doc number,...], ... }\n",
        "    test_a = []     # [ 1. text's author, ... ]\n",
        "    test_t = []     # [ 1. text's doc number, ... ]\n",
        "\n",
        "    temp = {}       # to keep random index to selecting which texts will be test text\n",
        "    for aut, number in authorsTextNumbers.items():\n",
        "        temp[aut] = []\n",
        "        while len(temp[aut]) < n:\n",
        "            x = random.randint(0, number-1)\n",
        "            if x not in temp[aut]:\n",
        "                temp[aut].append(x)\n",
        "        temp[aut] = sorted(temp[aut])\n",
        "\n",
        "    # temp = {'1': [16, 28, 29, 39, 47], '10': [0, 6, 19, 23, 24], ...}\n",
        "\n",
        "    for doc in dict.keys():\n",
        "        # doc number --> author, text number\n",
        "        author = doc.split(\".\")\n",
        "        author = author[0]\n",
        "\n",
        "        if author not in train:\n",
        "            train[author] = []\n",
        "\n",
        "        if len(train[author]) in temp[author]:\n",
        "            temp[author].remove(len(train[author]))\n",
        "            for j, num in enumerate(temp[author]):\n",
        "                temp[author][j] -= 1\n",
        "            test_t.append(doc)\n",
        "            test_a.append(author)   # to prediction\n",
        "            \n",
        "        else:\n",
        "            train[author].append(doc)\n",
        "\n",
        "    return train, test_a, test_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLvvbGORWS61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test_a, test_t = splitTrainTest(textsDocNumbers, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPlZaY-eWNBz",
        "colab_type": "code",
        "outputId": "c426dbf5-a739-4772-a53d-1ba90b859dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "print(\"len(train) =\",len(train))\n",
        "print(\"len(train['1']) =\",len(train[\"1\"]))\n",
        "print(\"len(train['10']) =\",len(train[\"10\"]))\n",
        "\n",
        "print(\"len(test_a) =\",len(test_a))\n",
        "print(\"len(test_t) =\",len(test_t))"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(train) = 30\n",
            "len(train['1']) = 45\n",
            "len(train['10']) = 45\n",
            "len(test_a) = 150\n",
            "len(test_t) = 150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0wcBrunMv8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def splitIntoWords(text):\n",
        "    text = text.strip()\n",
        "\n",
        "    text = text.replace(\"(\",\"\")\n",
        "    text = text.replace(\")\",\"\")\n",
        "\n",
        "    text = text.replace(\"”\",'\"')\n",
        "    text = text.replace(',\"','\"')\n",
        "    text = text.replace(\",\", \"\")\n",
        "    text = text.replace(\"\\n\", \".\")\n",
        "    text = text.replace(\"  \", \" \")\n",
        "\n",
        "    text = re.split(r'\\W', text.lower())\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CCyPEvdN4erf",
        "colab": {}
      },
      "source": [
        "def createDB(aut, docs, DB, n):\n",
        "\n",
        "    temp = []\n",
        "    for doc in docs:\n",
        "        text = textsDocNumbers[doc]\n",
        "        temp.append(splitIntoWords(text))\n",
        "\n",
        "    # DB = { author1: { word1: [ 'word1's count in author1's texts , {word2: 'word1 word2' pairs' count in author1's texts, ...} ], ... }, ...}\n",
        "    # DB = { \"1\": { \"this\": [5, {\"is\": 2, ...} ], ... }, \"2\": {}, ...}\n",
        "\n",
        "    DB[aut] = {}\n",
        "\n",
        "    for text in temp:\n",
        "        for i, word in enumerate(text):\n",
        "            if i == len(text)-n:\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                for j in range(n-1):\n",
        "                    if j == 0:\n",
        "                        continue\n",
        "                    word += \" \" + text[i+j]\n",
        "\n",
        "                if word not in DB[aut]:\n",
        "                    DB[aut][word] = []\n",
        "                    DB[aut][word].append(0)\n",
        "                    DB[aut][word].append({})\n",
        "                \n",
        "                after = text[i+n-1]         # next word\n",
        "                DB[aut][word][0] += 1       # word1's count in text\n",
        "\n",
        "                # DB[aut][word][1]  dictionary contains words that come after word1 as key and their counts as value\n",
        "\n",
        "                if after not in DB[aut][word][1]:\n",
        "                    DB[aut][word][1][after] = 1\n",
        "                else:\n",
        "                    DB[aut][word][1][after] += 1\n",
        "\n",
        "    return DB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHLgj7BY7p9w",
        "colab_type": "code",
        "outputId": "811b1841-6ba0-42b1-9769-06b049104a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Create DB\n",
        "\n",
        "n = int(input('Enter n: '))\n",
        "DB = {}\n",
        "for aut, docs in train.items():\n",
        "    DB = createDB(aut, docs, DB, n)\n",
        "\n",
        "# n = 2\n",
        "# print(DB[\"1\"])\n",
        "# {'there': [150, {'can': 5, 'is': 21, 'and': 1,...}], 'can': [132, {'be': 20, 'sometimes': 1, 'pull': 1, 't': 19,...], ...}\n",
        "\n",
        "# n = 3\n",
        "# print(DB[\"1\"])\n",
        "# {'there can': [5, {'be': 4, 'never': 1}], 'can be': [20, {'no': 3, 'done': 2,...}], ...}\n",
        "\n",
        "# n = 4\n",
        "# print(DB[\"1\"])\n",
        "# {'there can be': [4, {'no': 3, 'little': 1}], 'can be no': [3, {'doubt': 2, 'doubting': 1}], ...}"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter n: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEmlNLimJ6oL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "2399a2dc-bb84-4cf5-c135-ab148e0db398"
      },
      "source": [
        "if n == 2:\n",
        "    print(\"The number of 'united' in 1's texts:\\n\", DB[\"1\"][\"united\"][0])\n",
        "    print(\"The number of 'united kingdom' in 1's texts:\\n\", DB[\"1\"][\"united\"][1][\"kingdom\"])\n",
        "    print(\"The words and their numbers after 'united' in 1's texts:\\n\", DB[\"1\"][\"united\"][1])"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of 'united' in 1's texts:\n",
            " 12\n",
            "The number of 'united kingdom' in 1's texts:\n",
            " 5\n",
            "The words and their numbers after 'united' in 1's texts:\n",
            " {'kingdom': 5, 'states': 5, 'nations': 1, 'the': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrPyjKuLTVZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "\n",
        "for doc in test_t:\n",
        "    pr = {}     # to find best author prediction for text\n",
        "                # { author: prediction value, ... }  \n",
        "\n",
        "    txt = textsDocNumbers[doc]\n",
        "    words = splitIntoWords(txt)\n",
        "\n",
        "    for aut in train.keys():\n",
        "\n",
        "        pr[aut] = 0\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            if i == len(words)-n:\n",
        "                break\n",
        "\n",
        "            for j in range(n-1):\n",
        "                if j == 0:\n",
        "                    continue\n",
        "                word += \" \" + words[i+j]\n",
        "\n",
        "            after = words[i+n-1]      # next word\n",
        "\n",
        "            if word in DB[aut]:\n",
        "                cTotal = DB[aut][word][0]               # of \"word\" in train text\n",
        "                if after in DB[aut][word][1]:\n",
        "                    cPart = DB[aut][word][1][after]     # of \"word after\" in train text\n",
        "\n",
        "                    p = cPart / cTotal      # prediction\n",
        "                else:\n",
        "                    p = 1e-10\n",
        "            else:\n",
        "                p = 1e-10\n",
        "\n",
        "            # to prevent errors;\n",
        "            # if one of words(word or after) not in author's texts,\n",
        "            # p will equal very small number\n",
        "\n",
        "            pr[aut] += math.log(p)\n",
        "\n",
        "    # to find the best author prediction\n",
        "    pr = sorted(pr.items(), key=lambda pr: pr[1], reverse=True)\n",
        "    # pr[0][0] --> the best author\n",
        "    predictions.append(pr[0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sbOIROGcrpK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8e285045-107a-4e14-e43f-c7068ec2a607"
      },
      "source": [
        "print(predictions)"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1', '1', '1', '1', '1', '10', '10', '10', '10', '10', '9', '4', '11', '16', '16', '12', '12', '12', '12', '12', '13', '13', '13', '13', '13', '12', '14', '14', '14', '12', '15', '15', '12', '15', '15', '16', '16', '16', '16', '16', '17', '17', '17', '17', '17', '19', '19', '18', '18', '18', '19', '19', '19', '19', '19', '2', '2', '2', '2', '2', '20', '20', '20', '20', '20', '21', '21', '21', '21', '21', '22', '22', '22', '22', '22', '23', '23', '23', '23', '23', '24', '24', '24', '24', '24', '25', '25', '25', '25', '25', '26', '26', '26', '26', '26', '27', '27', '27', '27', '16', '12', '8', '12', '12', '28', '16', '10', '16', '16', '16', '3', '3', '10', '3', '3', '30', '17', '10', '30', '10', '4', '4', '4', '4', '4', '5', '5', '5', '5', '10', '10', '10', '6', '10', '10', '10', '10', '10', '10', '10', '12', '12', '8', '8', '8', '10', '11', '11', '9', '9']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Wza-Z9Ds5jE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3133a825-1cfc-4e7b-98b5-d7564beb37ce"
      },
      "source": [
        "acc = 0\n",
        "for i in range(len(predictions)):\n",
        "    if predictions[i] == test_a[i]:\n",
        "        acc += 1\n",
        "\n",
        "print(acc, \"/\", len(predictions))\n",
        "print(\"accuracy:\", (acc / len(predictions)))\n",
        "\n",
        "# n = 2\n",
        "# 105 / 150     ---> accuracy: 0.7\n",
        "# 113 / 150     ---> accuracy: 0.75 \n",
        "# 111 / 150     ---> accuracy: 0.74\n",
        "\n",
        "# n = 3\n",
        "# 111 / 150     ---> accuracy: 0.74\n",
        "\n",
        "# n = 4\n",
        "# 107 / 150     ---> accuracy: 0.71"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "112 / 150\n",
            "accuracy: 0.7466666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uwjOEaJfMqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "import sys\n",
        "\n",
        "def get_obj_size(obj):\n",
        "    marked = {id(obj)}\n",
        "    obj_q = [obj]\n",
        "    sz = 0\n",
        "\n",
        "    while obj_q:\n",
        "        sz += sum(map(sys.getsizeof, obj_q))\n",
        "\n",
        "        # Lookup all the object referred to by the object in obj_q.\n",
        "        # See: https://docs.python.org/3.7/library/gc.html#gc.get_referents\n",
        "        all_refr = ((id(o), o) for o in gc.get_referents(*obj_q))\n",
        "\n",
        "        # Filter object that are already marked.\n",
        "        # Using dict notation will prevent repeated objects.\n",
        "        new_refr = {o_id: o for o_id, o in all_refr if o_id not in marked and not isinstance(o, type)}\n",
        "\n",
        "        # The new obj_q will be the ones that were not marked,\n",
        "        # and we will update marked with their ids so we will\n",
        "        # not traverse them again.\n",
        "        obj_q = new_refr.values()\n",
        "        marked.update(new_refr.keys())\n",
        "\n",
        "    return sz\n",
        "\n",
        "# get_obj_size(DB)\n",
        "\n",
        "# n = 2\n",
        "# 71962948 = 71.96\n",
        "\n",
        "# n = 3\n",
        "# 252950532 = 252.95"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}